{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.core.utils import describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.loader.build_loader import build_dataloader\n",
    "from mmcv.utils.config import Config\n",
    "\n",
    "\n",
    "cfg = Config.fromfile('config/retinanet_x101_64x4d_fpn_1x.py')\n",
    "\n",
    "train_cfg = cfg.train_cfg\n",
    "test_cfg = cfg.test_cfg\n",
    "dataset_cfg = cfg.data.val\n",
    "\n",
    "loader = iter(build_dataloader(dataset_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extractor :  backbone + neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "backbone=dict(\n",
    "        type='ResNeXt',\n",
    "        depth=101,\n",
    "        groups=64,\n",
    "        base_width=4,\n",
    "        num_stages=4,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "        frozen_stages=1,\n",
    "        style='pytorch'),\n",
    "neck=dict(\n",
    "        type='FPN',\n",
    "        in_channels=[256, 512, 1024, 2048],\n",
    "        out_channels=256,\n",
    "        start_level=1,\n",
    "        add_extra_convs=True,\n",
    "        num_outs=5)\n",
    "\"\"\"\n",
    "from src.models.builder import build_backbone, build_neck\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        backbone_cfg = cfg.model.backbone\n",
    "        neck_cfg = cfg.model.neck\n",
    "        \n",
    "        self.backbone = build_backbone(backbone_cfg)\n",
    "        self.neck = build_neck(neck_cfg)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Raw Image shape : {describe(x)}\")\n",
    "        \n",
    "        feature = self.backbone(x)\n",
    "        print(f\"After Resnet Passed: {describe(feature)}\")\n",
    "        \n",
    "        multi_level_feature = self.neck(feature)\n",
    "        print(f\"After FPN Passed: {describe(multi_level_feature)}\")\n",
    "        \n",
    "        return multi_level_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head : RetinaHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bbox_head=dict(\n",
    "    type='RetinaHead',\n",
    "    num_classes=81,  # background + 80 (RetinaNet)\n",
    "    in_channels=256, # (RetinaNet)\n",
    "    stacked_convs=4,  # number of class/box subnet's conv layers (RetinaNet)\n",
    "    feat_channels=256,  # num_channels in subnet's conv feature (RetinaNet)\n",
    "    octave_base_scale=4,  # anchor scale related factor (RetinaNet)\n",
    "    scales_per_octave=3,  # anchor scale related factor (RetinaNet)\n",
    "    anchor_ratios=[0.5, 1.0, 2.0],  # anchor scale related factor (RetinaNet)\n",
    "    anchor_strides=[8, 16, 32, 64, 128],  # stride of anchor, normally stride of feature map. (RetinaNet)\n",
    "    target_means=[.0, .0, .0, .0],  # regression target mean (RetinaNet)\n",
    "    target_stds=[1.0, 1.0, 1.0, 1.0]))  # regression target std (RetinaNet)\n",
    "\"\"\"\n",
    "from src.models.builder import build_head\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.bbox_head = build_head(cfg.model.bbox_head)\n",
    "    \n",
    "    def forward(self, feature):\n",
    "        cls_score, bbox_pred = self.bbox_head(feature)\n",
    "        return cls_score, bbox_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "\n",
    "feature_extractor = FeatureExtractor(cfg)\n",
    "_ = load_checkpoint(feature_extractor, 'pretrained/retinanet_x101_64x4d_fpn_1x_pretrained.pth')\n",
    "\n",
    "bbox_head = Head(cfg)\n",
    "_ = load_checkpoint(bbox_head, 'pretrained/retinanet_x101_64x4d_fpn_1x_pretrained.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(loader)\n",
    "print(sample.keys())\n",
    "\n",
    "img = sample['img'].data[0]\n",
    "img_metas = sample['img_meta'].data[0]\n",
    "gt_bboxes = sample['gt_bboxes'].data[0]\n",
    "gt_labels = sample['gt_labels'].data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = feature_extractor(img)\n",
    "print(describe(feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bbox prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_score, bbox_pred = bbox_head(feature)\n",
    "\n",
    "print(describe(cls_score))\n",
    "print(describe(bbox_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define anchor_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_head.bbox_head.init_anchor_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.visualize import draw_base_anchor\n",
    "\n",
    "\n",
    "draw_base_anchor(bbox_head.bbox_head.anchor_generators[4], line_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.quiz.quiz1 import get_anchors\n",
    "\n",
    "featmap_sizes = [featmap.size()[-2:] for featmap in cls_score]\n",
    "anchor_list, valid_flag_list = get_anchors(bbox_head.bbox_head.anchor_generators, bbox_head.bbox_head.anchor_strides, featmap_sizes, img_metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(anchor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(valid_flag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = len(img_metas)\n",
    "assert len(anchor_list) == len(valid_flag_list) == num_imgs\n",
    "\n",
    "num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n",
    "\n",
    "for i in range(num_imgs):\n",
    "    assert len(anchor_list[i]) == len(valid_flag_list[i])\n",
    "    anchor_list[i] = torch.cat(anchor_list[i])\n",
    "    valid_flag_list[i] = torch.cat(valid_flag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_target_variable = (anchor_list, valid_flag_list, gt_bboxes, gt_labels, img_metas, train_cfg, bbox_head.bbox_head.cls_out_channels)\n",
    "%store anchor_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import multi_apply\n",
    "from src.core.anchor import anchor_target_single\n",
    "\n",
    "\n",
    "(all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,\n",
    "pos_inds_list, neg_inds_list) = multi_apply(\n",
    "   anchor_target_single,\n",
    "    anchor_list,\n",
    "    valid_flag_list,\n",
    "    gt_bboxes,\n",
    "    gt_labels,\n",
    "    img_metas,\n",
    "    target_means=[.0, .0, .0, .0],\n",
    "    target_stds=[1.0, 1.0, 1.0, 1.0],\n",
    "    cfg=train_cfg,\n",
    "    label_channels=bbox_head.bbox_head.cls_out_channels,\n",
    "    sampling=False,\n",
    "    unmap_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(describe(all_labels))\n",
    "print(describe(all_label_weights))\n",
    "print(describe(all_bbox_targets))\n",
    "print(describe(all_bbox_weights))\n",
    "print(describe(pos_inds_list))\n",
    "print(describe(neg_inds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.quiz.quiz2 import images_to_levels\n",
    "\n",
    "\"\"\"\n",
    "이미지별로 구성한 target을 다시 level별로 구성되도록 형태를 바꿔줍니다.\n",
    "\"\"\"\n",
    "labels_list = images_to_levels(all_labels, num_level_anchors)\n",
    "label_weights_list = images_to_levels(all_label_weights, num_level_anchors)\n",
    "bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)\n",
    "bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(describe(labels_list))\n",
    "print(describe(label_weights_list))\n",
    "print(describe(bbox_targets_list))\n",
    "print(describe(bbox_weights_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "positive / negative sample의 개수를 각각 들고있습니다.\n",
    "loss를 구하는 과정에서 normalize를 거치게 되며, 이는 곧 sample의 개수로 나누는 것을 의미합니다.\n",
    "\n",
    "RetinaNet은 positive sample만을 사용하므로 사실상 num_total_neg는 무의미합니다.\n",
    "\"\"\"\n",
    "from src.quiz.quiz3 import loss_single\n",
    "\n",
    "num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n",
    "num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n",
    "\n",
    "losses_cls, losses_reg = multi_apply(\n",
    "    loss_single,\n",
    "    cls_score,\n",
    "    bbox_pred,\n",
    "    labels_list,\n",
    "    label_weights_list,\n",
    "    bbox_targets_list,\n",
    "    bbox_weights_list,\n",
    "    num_total_samples=num_total_pos,\n",
    "    cfg=cfg.train_cfg,\n",
    "    cls_out_channels=bbox_head.bbox_head.cls_out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(describe(losses_cls))\n",
    "print(losses_cls)\n",
    "print(describe(losses_reg))\n",
    "print(losses_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.quiz.quiz8 import get_bboxes\n",
    "\n",
    "bbox_list = get_bboxes(cls_score, bbox_pred, img_metas, test_cfg,\n",
    "                       bbox_head.bbox_head.anchor_generators, bbox_head.bbox_head.anchor_strides, bbox_head.bbox_head.cls_out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import bbox2result\n",
    "\n",
    "bbox_results = [\n",
    "    bbox2result(det_bboxes, det_labels, bbox_head.bbox_head.num_classes)\n",
    "    for det_bboxes, det_labels in bbox_list\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw bbox on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.show_result import show_result\n",
    "\n",
    "show_result(sample, bbox_results[0], cfg.img_norm_cfg, 'coco')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycon",
   "language": "python",
   "name": "ub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
