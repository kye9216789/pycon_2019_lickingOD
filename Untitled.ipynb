{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Object Detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How ancestors did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How done recently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single stage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS COCO Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Anchor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train object detector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from src.anchor.anchor_generator import (gen_base_anchors, get_anchors, \n",
    "                              grid_anchors, meshgrid)\n",
    "from src.anchor.assigner import assign_wrt_overlaps, bbox_overlaps\n",
    "from src.anchor.loss import binary_cross_entropy, smooth_l1_loss\n",
    "from src.anchor.prediction import predict_anchors\n",
    "from src.anchor.transforms import bbox2delta, delta2bbox\n",
    "from src.anchor.visualize import (draw_anchor_gt_overlaps, draw_anchor_samples_on_image, \n",
    "                       draw_base_anchor_on_grid, draw_pos_assigned_bboxes)\n",
    "from src.datasets.loader.build_loader import build_dataloader\n",
    "from src.models.builder import build_backbone, build_neck, build_head\n",
    "from mmcv.runner import obj_from_dict\n",
    "from mmcv.utils.config import Config\n",
    "from src.core import multi_apply, weighted_smoothl1, weighted_sigmoid_focal_loss\n",
    "from src.core.anchor import anchor_target_single, images_to_levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.80s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('config/retinanet_r50_fpn_1x.py')\n",
    "\n",
    "train_cfg = cfg.train_cfg\n",
    "dataset_cfg = cfg.data.train\n",
    "\n",
    "loader = iter(build_dataloader(dataset_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        backbone_cfg = dict(\n",
    "                type='ResNet',\n",
    "                depth=50,\n",
    "                num_stages=4,\n",
    "                out_indices=(0, 1, 2, 3),\n",
    "                frozen_stages=1,\n",
    "                style='pytorch')\n",
    "\n",
    "        neck_cfg =dict(\n",
    "                type='FPN',\n",
    "                in_channels=[256, 512, 1024, 2048],\n",
    "                out_channels=256,\n",
    "                start_level=1,\n",
    "                add_extra_convs=True,\n",
    "                num_outs=5)\n",
    "        self.resnet_backbone = build_backbone(backbone_cfg)\n",
    "        self.fpn_neck = build_neck(neck_cfg)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        feature = self.resnet_backbone(x)\n",
    "        multi_level_feature = self.fpn_neck(feature)\n",
    "        print(len(feature))\n",
    "        print(len(multi_level_feature))\n",
    "        print([x.shape for x in feature])\n",
    "        print([x.shape for x in multi_level_feature])\n",
    "        return multi_level_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo : sample dict의 key목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768, 1344])\n",
      "4\n",
      "5\n",
      "[torch.Size([1, 256, 192, 336]), torch.Size([1, 512, 96, 168]), torch.Size([1, 1024, 48, 84]), torch.Size([1, 2048, 24, 42])]\n",
      "[torch.Size([1, 256, 96, 168]), torch.Size([1, 256, 48, 84]), torch.Size([1, 256, 24, 42]), torch.Size([1, 256, 12, 21]), torch.Size([1, 256, 6, 11])]\n"
     ]
    }
   ],
   "source": [
    "sample = next(loader)\n",
    "img = sample['img'].data[0]\n",
    "img_metas = sample['img_meta'].data[0]\n",
    "gt_bboxes = sample['gt_bboxes'].data[0]\n",
    "gt_labels = sample['gt_labels'].data[0]\n",
    "\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature = feature_extractor(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build rpn head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_config = cfg.model.bbox_head\n",
    "\n",
    "\"\"\"\n",
    "dict(\n",
    "    type='RetinaHead',\n",
    "    num_classes=81,\n",
    "    in_channels=256,\n",
    "    stacked_convs=4,\n",
    "    feat_channels=256,\n",
    "    octave_base_scale=4,\n",
    "    scales_per_octave=3,\n",
    "    anchor_ratios=[0.5, 1.0, 2.0],\n",
    "    anchor_strides=[8, 16, 32, 64, 128],\n",
    "    target_means=[.0, .0, .0, .0],\n",
    "    target_stds=[1.0, 1.0, 1.0, 1.0])\n",
    "\"\"\"\n",
    "rpn_head = build_head(rpn_config)\n",
    "cls_score, bbox_pred = rpn_head(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 720, 96, 168]), torch.Size([1, 720, 48, 84]), torch.Size([1, 720, 24, 42]), torch.Size([1, 720, 12, 21]), torch.Size([1, 720, 6, 11])]\n",
      "[torch.Size([1, 36, 96, 168]), torch.Size([1, 36, 48, 84]), torch.Size([1, 36, 24, 42]), torch.Size([1, 36, 12, 21]), torch.Size([1, 36, 6, 11])]\n"
     ]
    }
   ],
   "source": [
    "print([x.shape for x in cls_score])\n",
    "print([x.shape for x in bbox_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define anchor_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_head.init_anchor_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(anchor_generators, anchor_strides, featmap_sizes, img_metas):\n",
    "    \"\"\"Get anchors according to feature map sizes.\n",
    "\n",
    "    Args:\n",
    "        featmap_sizes (list[tuple]): Multi-level feature map sizes.\n",
    "        img_metas (list[dict]): Image meta info.\n",
    "\n",
    "    Returns:\n",
    "        tuple: anchors of each image, valid flags of each image\n",
    "    \"\"\"\n",
    "    num_imgs = len(img_metas)\n",
    "    num_levels = len(featmap_sizes)\n",
    "\n",
    "    # since feature map sizes of all images are the same, we only compute\n",
    "    # anchors for one time\n",
    "    multi_level_anchors = []\n",
    "    for i in range(num_levels):\n",
    "        anchors = anchor_generators[i].grid_anchors(\n",
    "            featmap_sizes[i], anchor_strides[i], device='cpu')\n",
    "        multi_level_anchors.append(anchors)\n",
    "    anchor_list = [multi_level_anchors for _ in range(num_imgs)]\n",
    "\n",
    "    # for each image, we compute valid flags of multi level anchors\n",
    "    valid_flag_list = []\n",
    "    for img_id, img_meta in enumerate(img_metas):\n",
    "        multi_level_flags = []\n",
    "        for i in range(num_levels):\n",
    "            anchor_stride = anchor_strides[i]\n",
    "            feat_h, feat_w = featmap_sizes[i]\n",
    "            h, w, _ = img_meta['pad_shape']\n",
    "            valid_feat_h = min(int(np.ceil(h / anchor_stride)), feat_h)\n",
    "            valid_feat_w = min(int(np.ceil(w / anchor_stride)), feat_w)\n",
    "            flags = anchor_generators[i].valid_flags(\n",
    "                (feat_h, feat_w), (valid_feat_h, valid_feat_w))\n",
    "            multi_level_flags.append(flags)\n",
    "        valid_flag_list.append(multi_level_flags)\n",
    "\n",
    "    return anchor_list, valid_flag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featmap_sizes = [featmap.size()[-2:] for featmap in cls_score]\n",
    "anchor_list, valid_flag_list = get_anchors(rpn_head.anchor_generators, rpn_head.anchor_strides, featmap_sizes, img_metas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anchor를 통해 target 후보를 만들고\n",
    "#### sample을 통해 유의미한 target을 골라낸다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo : anchor_target 해체, anchor_target_single 가져오고 문제거리 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = len(img_metas)\n",
    "assert len(anchor_list) == len(valid_flag_list) == num_imgs\n",
    "\n",
    "num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n",
    "\n",
    "for i in range(num_imgs):\n",
    "    assert len(anchor_list[i]) == len(valid_flag_list[i])\n",
    "    anchor_list[i] = torch.cat(anchor_list[i])\n",
    "    valid_flag_list[i] = torch.cat(valid_flag_list[i])\n",
    "    \n",
    "gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n",
    "gt_labels_list = [None for _ in range(num_imgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'anchor_target_variable' (tuple)\n"
     ]
    }
   ],
   "source": [
    "anchor_target_variable = (anchor_list, valid_flag_list, gt_bboxes, gt_bboxes_ignore_list, gt_labels_list, img_metas, train_cfg)\n",
    "%store anchor_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,\n",
    "pos_inds_list, neg_inds_list) = multi_apply(\n",
    "   anchor_target_single,\n",
    "    anchor_list,\n",
    "    valid_flag_list,\n",
    "    gt_bboxes,\n",
    "    gt_bboxes_ignore_list,\n",
    "    gt_labels_list,\n",
    "    img_metas,\n",
    "    target_means=[.0, .0, .0, .0],\n",
    "    target_stds=[1.0, 1.0, 1.0, 1.0],\n",
    "    cfg=train_cfg,\n",
    "    label_channels=rpn_head.cls_out_channels,\n",
    "    sampling=False,\n",
    "    unmap_outputs=True)\n",
    "\n",
    "# no valid anchors\n",
    "if any([labels is None for labels in all_labels]):\n",
    "    assert False\n",
    "\n",
    "# sampled anchors of all images\n",
    "num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n",
    "num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n",
    "# split targets to a list w.r.t. multiple levels\n",
    "labels_list = images_to_levels(all_labels, num_level_anchors)\n",
    "label_weights_list = images_to_levels(all_label_weights, num_level_anchors)\n",
    "bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)\n",
    "bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo : assign & sample용 notebook 따로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo : loss_single 가져오고 문제로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_single(cls_score, bbox_pred, labels, label_weights,\n",
    "                bbox_targets, bbox_weights, num_total_samples, cfg, cls_out_channels):\n",
    "    # classification loss\n",
    "    labels = labels.reshape(-1, cls_out_channels)\n",
    "    label_weights = label_weights.reshape(-1, cls_out_channels)\n",
    "    cls_score = cls_score.permute(0, 2, 3, 1).reshape(\n",
    "        -1, cls_out_channels)\n",
    "    loss_cls = weighted_sigmoid_focal_loss(\n",
    "        cls_score,\n",
    "        labels,\n",
    "        label_weights,\n",
    "        gamma=cfg.gamma,\n",
    "        alpha=cfg.alpha,\n",
    "        avg_factor=num_total_samples)\n",
    "\n",
    "    # regression loss\n",
    "    bbox_targets = bbox_targets.reshape(-1, 4)\n",
    "    bbox_weights = bbox_weights.reshape(-1, 4)\n",
    "    bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n",
    "    loss_reg = weighted_smoothl1(\n",
    "        bbox_pred,\n",
    "        bbox_targets,\n",
    "        bbox_weights,\n",
    "        beta=cfg.smoothl1_beta,\n",
    "        avg_factor=num_total_samples)\n",
    "    return loss_cls, loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_cls, losses_reg = multi_apply(\n",
    "    loss_single,\n",
    "    cls_score,\n",
    "    bbox_pred,\n",
    "    labels_list,\n",
    "    label_weights_list,\n",
    "    bbox_targets_list,\n",
    "    bbox_weights_list,\n",
    "    num_total_samples=num_total_pos,\n",
    "    cfg=cfg.train_cfg,\n",
    "    cls_out_channels=rpn_head.cls_out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([12832.7061], grad_fn=<DivBackward0>),\n",
       " tensor([3182.1497], grad_fn=<DivBackward0>),\n",
       " tensor([793.8104], grad_fn=<DivBackward0>),\n",
       " tensor([194.2783], grad_fn=<DivBackward0>),\n",
       " tensor([50.7037], grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2658], grad_fn=<DivBackward0>),\n",
       " tensor([0.4994], grad_fn=<DivBackward0>),\n",
       " tensor([0.], grad_fn=<DivBackward0>),\n",
       " tensor([0.1951], grad_fn=<DivBackward0>),\n",
       " tensor([0.0294], grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(cls_scores, bbox_preds, img_metas, cfg,\n",
    "                anchor_generators, anchor_strides, cls_out_channels):\n",
    "    assert len(cls_scores) == len(bbox_preds)\n",
    "    num_levels = len(cls_scores)\n",
    "\n",
    "    mlvl_anchors = [\n",
    "        anchor_generators[i].grid_anchors(cls_scores[i].size()[-2:],\n",
    "                                            anchor_strides[i], device='cpu')\n",
    "        for i in range(num_levels)\n",
    "    ]\n",
    "    result_list = []\n",
    "    for img_id in range(len(img_metas)):\n",
    "        cls_score_list = [\n",
    "            cls_scores[i][img_id].detach() for i in range(num_levels)\n",
    "        ]\n",
    "        bbox_pred_list = [\n",
    "            bbox_preds[i][img_id].detach() for i in range(num_levels)\n",
    "        ]\n",
    "        img_shape = img_metas[img_id]['img_shape']\n",
    "        scale_factor = img_metas[img_id]['scale_factor']\n",
    "        proposals = get_bboxes_single(cls_score_list, bbox_pred_list,\n",
    "                                        mlvl_anchors, img_shape,\n",
    "                                        scale_factor, cfg, cls_out_channels)\n",
    "        result_list.append(proposals)\n",
    "    return result_list\n",
    "\n",
    "def get_bboxes_single(cls_scores,\n",
    "                      bbox_preds,\n",
    "                      mlvl_anchors,\n",
    "                      img_shape,\n",
    "                      scale_factor,\n",
    "                      cfg,\n",
    "                      cls_out_channels,\n",
    "                      target_means=[.0, .0, .0, .0],\n",
    "                      target_stds=[1.0, 1.0, 1.0, 1.0]):\n",
    "    assert len(cls_scores) == len(bbox_preds) == len(mlvl_anchors)\n",
    "    mlvl_bboxes = []\n",
    "    mlvl_scores = []\n",
    "    for cls_score, bbox_pred, anchors in zip(cls_scores, bbox_preds,\n",
    "                                                mlvl_anchors):\n",
    "        assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n",
    "        cls_score = cls_score.permute(1, 2, 0).reshape(\n",
    "            -1, cls_out_channels)\n",
    "\n",
    "        scores = cls_score.sigmoid()\n",
    "\n",
    "        bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n",
    "        nms_pre = cfg.get('nms_pre', -1)\n",
    "        if nms_pre > 0 and scores.shape[0] > nms_pre:\n",
    "\n",
    "            max_scores, _ = scores.max(dim=1)\n",
    "            _, topk_inds = max_scores.topk(nms_pre)\n",
    "            anchors = anchors[topk_inds, :]\n",
    "            bbox_pred = bbox_pred[topk_inds, :]\n",
    "            scores = scores[topk_inds, :]\n",
    "        bboxes = delta2bbox(anchors, bbox_pred, target_means,\n",
    "                            target_stds, img_shape)\n",
    "        mlvl_bboxes.append(bboxes)\n",
    "        mlvl_scores.append(scores)\n",
    "    mlvl_bboxes = torch.cat(mlvl_bboxes)\n",
    "    mlvl_scores = torch.cat(mlvl_scores)\n",
    "\n",
    "    padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n",
    "    mlvl_scores = torch.cat([padding, mlvl_scores], dim=1)\n",
    "    det_bboxes, det_labels = multiclass_nms(\n",
    "        mlvl_bboxes, mlvl_scores, cfg.score_thr, cfg.nms, cfg.max_per_img)\n",
    "    return det_bboxes, det_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multiclass_nms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7f1e069fbae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m bbox_list = get_bboxes(cls_score, bbox_pred, img_metas, test_cfg,\n\u001b[0;32m----> 2\u001b[0;31m                        rpn_head.anchor_generators, rpn_head.anchor_strides, rpn_head.cls_out_channels)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-30fe5d2e9453>\u001b[0m in \u001b[0;36mget_bboxes\u001b[0;34m(cls_scores, bbox_preds, img_metas, cfg, anchor_generators, anchor_strides, cls_out_channels)\u001b[0m\n\u001b[1;32m     21\u001b[0m         proposals = get_bboxes_single(cls_score_list, bbox_pred_list,\n\u001b[1;32m     22\u001b[0m                                         \u001b[0mmlvl_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                         scale_factor, cfg, cls_out_channels)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mresult_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-30fe5d2e9453>\u001b[0m in \u001b[0;36mget_bboxes_single\u001b[0;34m(cls_scores, bbox_preds, mlvl_anchors, img_shape, scale_factor, cfg, cls_out_channels, target_means, target_stds)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlvl_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlvl_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmlvl_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlvl_scores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     det_bboxes, det_labels = multiclass_nms(\n\u001b[0m\u001b[1;32m     66\u001b[0m         mlvl_bboxes, mlvl_scores, cfg.score_thr, cfg.nms, cfg.max_per_img)\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdet_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multiclass_nms' is not defined"
     ]
    }
   ],
   "source": [
    "bbox_list = get_bboxes(cls_score, bbox_pred, img_metas, test_cfg,\n",
    "                       rpn_head.anchor_generators, rpn_head.anchor_strides, rpn_head.cls_out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bbox_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3183c2ba2f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m bbox_results = [\n\u001b[1;32m      2\u001b[0m     \u001b[0mbbox2result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdet_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbbox_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m ][0]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bbox_list' is not defined"
     ]
    }
   ],
   "source": [
    "bbox_results = [\n",
    "    bbox2result(det_bboxes, det_labels, rpn_head.num_classes)\n",
    "    for det_bboxes, det_labels in bbox_list\n",
    "][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycon",
   "language": "python",
   "name": "ub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
